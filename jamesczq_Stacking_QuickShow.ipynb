{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global Setting\n",
    "#-----------------------------------------------------\n",
    "\n",
    "# Rand Seed\n",
    "RandSeed0 = 123;\n",
    "\n",
    "# Set the threshold for being over-skewed\n",
    "# (can tune later)\n",
    "skew0 = 0.7;\n",
    "\n",
    "# Set the number of folds in the K-fold CV\n",
    "num_of_folds = 5;\n",
    "#-----------------------------------------------------\n",
    "\n",
    "\n",
    "# Setting of Flags\n",
    "#-----------------------------------------------------\n",
    "\n",
    "# For Section 1: \n",
    "# Data Loading\n",
    "#whichDataSet = 0;\n",
    "whichDataSet = 'w';\n",
    "\n",
    "# For Section 2: \n",
    "# Data Pre-Processing \n",
    "use_2_CookedUp_Features = 1;\n",
    "# Use Bath/Bed and Garage/Bed or not\n",
    "# 1 means Yes; other numbers mean No. \n",
    "\n",
    "use_2_CookedUp_Features_instead = 0;\n",
    "# Use Bath/Bed and Garage/Bed instead of Bath (Full, Half), Garage\n",
    "# 1 means Yes; other numbers mean No. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pylab \n",
    "import scipy.stats as stats\n",
    "\n",
    "from scipy.stats import skew\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import xgboost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test mega dataset shape: (2919, 72)\n"
     ]
    }
   ],
   "source": [
    "if whichDataSet == 0: # Using the original data set \n",
    "    train = pd.read_csv('Datasets/train.csv')\n",
    "    test = pd.read_csv('Datasets/test.csv')\n",
    "    # Concat. train[no ID column, ... (all columns) ..., no SalePrice column]\n",
    "    #     with test[no ID column, ... (all columns) ..., no SalePrice column]\n",
    "    # (Test data has no SalePrice column anyway)\n",
    "    train_test = pd.concat([train.loc[:,'MSSubClass':'SaleCondition'],\n",
    "                             test.loc[:,'MSSubClass':'SaleCondition']]);\n",
    "    print('Train-Test mega dataset shape:',train_test.shape)\n",
    "##\n",
    "##\n",
    "if whichDataSet == 'w': # Using Wenchang's modified data set\n",
    "    train = pd.read_csv('Datasets/train_Wenchang.csv')\n",
    "    test = pd.read_csv('Datasets/test_Wenchang.csv')\n",
    "    train.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    test.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    train_test = pd.concat([train.loc[:,'1stFlrSF':'RemodYearDiff'],\n",
    "                             test.loc[:,'1stFlrSF':'RemodYearDiff']]);\n",
    "    print('Train-Test mega dataset shape:',train_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Pre-Processing\n",
    "* 2.0 (Optional) Add Cooked_up Features: Bath_Capacity and Parking_Capacity\n",
    "* 2.1 Some Preliminary Examination and Symmetrization\n",
    "* 2.2 Fill in NAs\n",
    "* 2.3 Encode Categorical Features\n",
    "* 2.4 Set up training and test data matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 (Optional) Add Cooked_up Features: Bath_Capacitance and Parking_Capacitance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if use_2_CookedUp_Features == 1:\n",
    "    #\n",
    "    # Compute Total Bathrooms in a house\n",
    "    # and set those with zero bathroom with median bathroom number\n",
    "    train['TotBath'] = train.FullBath + 0.5*train.HalfBath;\n",
    "    test['TotBath'] = test.FullBath + 0.5*test.HalfBath;\n",
    "    train_test['TotBath'] = train_test.FullBath + 0.5*train_test.HalfBath;\n",
    "    #\n",
    "    train['TotBath'].replace(0, train['TotBath'].median(), inplace=True);\n",
    "    test['TotBath'].replace(0, test['TotBath'].median(), inplace=True);\n",
    "    train_test['TotBath'].replace(0, train_test['TotBath'].median(), inplace=True);\n",
    "    #------------------------------------------------------------------------------\n",
    "    #\n",
    "    # Set those with zero bedroom with median bedfroom number \n",
    "    # (or set them to one to be conservative)\n",
    "    train['BedroomAbvGr'].replace(0, train['BedroomAbvGr'].median(), inplace=True);\n",
    "    test['BedroomAbvGr'].replace(0, test['BedroomAbvGr'].median(), inplace=True);\n",
    "    train_test['BedroomAbvGr'].replace(0, train_test['BedroomAbvGr'].median(), inplace=True);\n",
    "    #------------------------------------------------------------------------------\n",
    "    #\n",
    "    # Cook-up Feature 1:\n",
    "    # Bath_Capacitance = TotBath / BedroomAbvGr\n",
    "    train['Bath_Capacitance'] = train.TotBath / train.BedroomAbvGr\n",
    "    test['Bath_Capacitance'] = train.TotBath / train.BedroomAbvGr\n",
    "    train_test['Bath_Capacitance'] = train.TotBath / train.BedroomAbvGr\n",
    "    #------------------------------------------------------------------------------\n",
    "    #\n",
    "    # Cook-up Feature 2: \n",
    "    # Parking_Capacitance = TotBath / BedroomAbvGr\n",
    "    train['Parking_Capacitance'] = train.GarageCars / train.BedroomAbvGr\n",
    "    test['Parking_Capacitance'] = train.GarageCars / train.BedroomAbvGr\n",
    "    train_test['Parking_Capacitance'] = train.GarageCars / train.BedroomAbvGr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if use_2_CookedUp_Features_instead == 1:\n",
    "    #\n",
    "    # Reasonable dropping ---------------------------------------- \n",
    "    #\n",
    "    train.drop(train[['FullBath', \n",
    "                      'HalfBath']], axis=1, inplace=True)\n",
    "    test.drop(test[['FullBath', \n",
    "                    'HalfBath']], axis=1, inplace=True)\n",
    "    train_test.drop(train_test[['FullBath', \n",
    "                                'HalfBath']], axis=1, inplace=True)\n",
    "    #\n",
    "    # Dropping that needs more scrutiny ---------------------------------------- \n",
    "    #\n",
    "    #train.drop(train[['TotBath']], axis=1, inplace=True)\n",
    "    #test.drop(test[['TotBath']], axis=1, inplace=True)\n",
    "    #train_test.drop(train_test[['TotBath']], axis=1, inplace=True)\n",
    "    # Dropping that needs more scrutiny ---------------------------------------- \n",
    "    #\n",
    "    #train.drop(train[['GarageCars']], axis=1, inplace=True)\n",
    "    #test.drop(test[['GarageCars']], axis=1, inplace=True)\n",
    "    #train_test.drop(train_test[['GarageCars']], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 (a) Preliminary Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As observed, symmetrize SalePrice via log(1 + ***)\n",
    "train.SalePrice = np.log(1 + train.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 (b) Symmetrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric_features = train_test.dtypes[train_test.dtypes != \"object\"].index\n",
    "\n",
    "skewed_features = train[numeric_features].apply(lambda x: skew(x.dropna())) \n",
    "skewed_features = skewed_features[abs( skewed_features ) > skew0]\n",
    "skewed_features = skewed_features.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test[skewed_features] = np.log(1 + train_test[skewed_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Encode Categorical Features\n",
    "#### Using plain and simple one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test = pd.get_dummies(train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_test.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Fill in NAs\n",
    "\n",
    "#### Interpolating NAs with the median of each field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if whichDataSet == 0:\n",
    "    train_test = train_test.fillna(train_test.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Set up training and test data matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train_test[:train.shape[0]]\n",
    "X_test  = train_test[train.shape[0]:]\n",
    "y_train = train.SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Create Artificial Train-Test Data Sets via Train-Test-Split\n",
    "### For model validation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [X_train_v, X_test_v, y_train_v, y_test_v] \n",
    "# all come from the original [X_train, y_train] \n",
    "##\n",
    "## Naming convention: \n",
    "## The \"_v\" in names such as \"X_train_v\" is \n",
    "## to indicate such set is for validation purposes.\n",
    "\n",
    "X_train_v, X_test_v, y_train_v, y_test_v =\\\n",
    "    ms.train_test_split(deepcopy(X_train),\\\n",
    "                        deepcopy(y_train),\\\n",
    "                        test_size = 1/8,\\\n",
    "                        random_state = RandSeed0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5642.0\n",
      "334.0\n"
     ]
    }
   ],
   "source": [
    "print(-1 + np.exp(X_train_v.GrLivArea.max()))\n",
    "print(-1 + np.exp(X_train_v.GrLivArea.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using the [X_train_v, y_train_v] to search for the optimal hyper-parameter(s)\n",
    "def rmse_cv(model):\n",
    "    kf = KFold(num_of_folds, shuffle=True, random_state=42).get_n_splits(X_train)\n",
    "    rmse = np.sqrt( -cross_val_score(model, X_train, y_train, \n",
    "                                     scoring=\"neg_mean_squared_error\",\n",
    "                                     cv=kf))\n",
    "    return(rmse)\n",
    "\n",
    "\n",
    "# Use the built-in MSE calculator\n",
    "def rmse(y_predicted, y_actual):\n",
    "    return( np.sqrt( mean_squared_error(y_actual, y_predicted) ) )\n",
    "\n",
    "\n",
    "def R2(y_predicted, y_actual):\n",
    "    # R^2 = 1 - SS_residual / SS_total\n",
    "    SS_residual = sum((y_predicted - y_actual)**2)\n",
    "    SS_total = sum((y_actual - y_actual.mean())**2)\n",
    "    R2 = 1 - SS_residual / SS_total\n",
    "    return(R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Linear Regression with Lasso Regularizations where $L(\\ \\vec{\\beta} \\ ) = MSE + \\alpha \\cdot ||\\ \\vec{\\beta}\\ ||_{L_{1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "By CV, Lasso alpha set to 0.00025\n",
      "**************************************************\n",
      "Lasso Training Performace: R^2 = 0.9285\n",
      "**************************************************\n",
      "Lasso Training Performace: RMSE = 0.1068\n",
      "**************************************************\n",
      "Lasso Test Performace: R^2 = 0.9503\n",
      "**************************************************\n",
      "Lasso Test Performace: RMSE = 0.0874\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# supply several alphas to do CV\n",
    "alpha_array = np.linspace(10,0.00025,64)\n",
    "\n",
    "cv_Lasso = [rmse_cv( Lasso(alpha = Alpha) ).mean() for Alpha in alpha_array]\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "cv_Lasso = pd.Series(cv_Lasso, index = alpha_array)\n",
    "\n",
    "alpha0_Lasso = cv_Lasso[cv_Lasso == cv_Lasso.min()].index[0];\n",
    "rmse0 = cv_Lasso.min();\n",
    "\n",
    "print('*'*50)\n",
    "print('By CV, Lasso alpha set to {}'.format(alpha0_Lasso))\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Training the model\n",
    "model_Lasso = Lasso(alpha0_Lasso).fit(X_train, y_train);\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Training Performance\n",
    "predictions_Lasso_train = pd.DataFrame({\"Predicted\":model_Lasso.predict(X_train), \n",
    "                                        \"Actual\":y_train});\n",
    "predictions_Lasso_train[\"Residual\"] = predictions_Lasso_train.Actual - predictions_Lasso_train.Predicted;\n",
    "\n",
    "R2_Lasso_train = model_Lasso.score(X_train, y_train);\n",
    "\n",
    "RMSE_Lasso_train = rmse(predictions_Lasso_train.Actual, predictions_Lasso_train.Predicted);\n",
    "\n",
    "print('*'*50)\n",
    "print('Lasso Training Performace: R^2 = {:.4f}'.format(R2_Lasso_train))\n",
    "print('*'*50)\n",
    "print('Lasso Training Performace: RMSE = {:.4f}'.format(RMSE_Lasso_train))\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Test Performance\n",
    "predictions_Lasso_test = pd.DataFrame({\"Predicted\":model_Lasso.predict(X_test_v), \n",
    "                                       \"Actual\":y_test_v});\n",
    "predictions_Lasso_test[\"Residual\"] = predictions_Lasso_test.Actual - predictions_Lasso_test.Predicted;\n",
    "\n",
    "R2_Lasso_test = model_Lasso.score(X_test_v, y_test_v);\n",
    "\n",
    "RMSE_Lasso_test = rmse(predictions_Lasso_test.Actual, predictions_Lasso_test.Predicted);\n",
    "\n",
    "print('*'*50)\n",
    "print('Lasso Test Performace: R^2 = {:.4f}'.format(R2_Lasso_test))\n",
    "print('*'*50)\n",
    "print('Lasso Test Performace: RMSE = {:.4f}'.format(RMSE_Lasso_test))\n",
    "print('*'*50)\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Generate Test Vector (Ridge)\n",
    "\n",
    "output_Lasso_test = model_Lasso.predict(X_test);\n",
    "\n",
    "# de-Logarithm\n",
    "output_Lasso_test = -1 + np.exp(output_Lasso_test); \n",
    "\n",
    "#print(len(output_Lasso_test))\n",
    "\n",
    "#output_Lasso_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "By CV, parameters of XGBooster:\n",
      "learning_rate set to 0.1\n",
      "n_estimators set to 500\n",
      "min_child_weight set to 2\n",
      "**************************************************\n",
      "XGBooster Training Performace: R^2 = 0.9883\n",
      "**************************************************\n",
      "XGBooster Training Performace: RMSE = 0.0433\n",
      "**************************************************\n",
      "XGBooster Test Performace: R^2 = 0.9898\n",
      "**************************************************\n",
      "XGBooster Test Performace: RMSE = 0.0396\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "#xgb_paramters = {'learning_rate':[0.005, 0.01, 0.05, 0.1, 0.5],\n",
    "#                 'n_estimators':[50, 100, 200, 500]}\n",
    "\n",
    "#xgb_GridSearch = GridSearchCV(xgboost.XGBRegressor(), param_grid=xgb_paramters)\n",
    "\n",
    "#xgb_GridSearch.fit(X_train, y_train)\n",
    "\n",
    "#xgb_GridSearch.best_score_\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "#learning_rate0_xgb = xgb_GridSearch.best_params_['learning_rate'];\n",
    "#n_estimators0_xgb = xgb_GridSearch.best_params_['n_estimators'];\n",
    "\n",
    "learning_rate0_xgb = 0.1;\n",
    "n_estimators0_xgb = 500;\n",
    "min_child_weight0 = 2;\n",
    "\n",
    "print('*'*50)\n",
    "print('By CV, parameters of XGBooster:')\n",
    "print('learning_rate set to {}'.format(learning_rate0_xgb))\n",
    "print('n_estimators set to {}'.format(n_estimators0_xgb))\n",
    "print('min_child_weight set to {}'.format(min_child_weight0))\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "xgboost.XGBRegressor(\n",
    "    max_depth=3, \n",
    "    learning_rate=0.1, \n",
    "    n_estimators=100, \n",
    "    silent=True, \n",
    "    objective='reg:linear', \n",
    "    booster='gbtree', \n",
    "    n_jobs=1, \n",
    "    nthread=None, \n",
    "    gamma=0, \n",
    "    min_child_weight=1, \n",
    "    max_delta_step=0, \n",
    "    subsample=1, \n",
    "    colsample_bytree=1, \n",
    "    colsample_bylevel=1, \n",
    "    reg_alpha=0, \n",
    "    reg_lambda=1, \n",
    "    scale_pos_weight=1, \n",
    "    base_score=0.5, \n",
    "    random_state=0, \n",
    "    seed=None, \n",
    "    missing=None, \n",
    "    **kwargs)\n",
    "'''\n",
    "\n",
    "\n",
    "model_xgb = xgboost.XGBRegressor(learning_rate=learning_rate0_xgb,\n",
    "                                 n_estimators=n_estimators0_xgb,\n",
    "                                 min_child_weight=min_child_weight0);\n",
    "\n",
    "model_xgb = model_xgb.fit(X_train, y_train);\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Training Performance\n",
    "# The results are put in dataframe \"predictions_xgb_train\"\n",
    "\n",
    "predictions_xgb_train = pd.DataFrame({\"Predicted\":model_xgb.predict(X_train), \n",
    "                                      \"Actual\":y_train});\n",
    "predictions_xgb_train[\"Residual\"] = predictions_xgb_train.Actual - predictions_xgb_train.Predicted;\n",
    "\n",
    "R2_xgb_train = R2(predictions_xgb_train.Predicted, predictions_xgb_train.Actual);\n",
    "\n",
    "RMSE_xgb_train = rmse(predictions_xgb_train.Actual, predictions_xgb_train.Predicted);\n",
    "\n",
    "print('*'*50)\n",
    "print('XGBooster Training Performace: R^2 = {:.4f}'.format(R2_xgb_train))\n",
    "print('*'*50)\n",
    "print('XGBooster Training Performace: RMSE = {:.4f}'.format(RMSE_xgb_train))\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Test Performance\n",
    "# The results are put in dataframe \"predictions_xgb_test\"\n",
    "\n",
    "predictions_xgb_test = pd.DataFrame({\"Predicted\":model_xgb.predict(X_test_v), \n",
    "                                      \"Actual\":y_test_v});\n",
    "predictions_xgb_test[\"Residual\"] = predictions_xgb_test.Actual - predictions_xgb_test.Predicted;\n",
    "\n",
    "\n",
    "R2_xgb_test = R2(predictions_xgb_test.Predicted, predictions_xgb_test.Actual);\n",
    "\n",
    "RMSE_xgb_test = rmse(predictions_xgb_test.Actual, predictions_xgb_test.Predicted);\n",
    "\n",
    "print('*'*50)\n",
    "print('XGBooster Test Performace: R^2 = {:.4f}'.format(R2_xgb_test));\n",
    "print('*'*50)\n",
    "print('XGBooster Test Performace: RMSE = {:.4f}'.format(RMSE_xgb_test));\n",
    "print('*'*50)\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Generate Test Vector (XGB)\n",
    "\n",
    "output_xgb_test = model_xgb.predict(X_test);\n",
    "\n",
    "# de-Logarithm\n",
    "output_xgb_test = -1 + np.exp(output_xgb_test); \n",
    "\n",
    "#print(len(output_xgb_test))\n",
    "\n",
    "#output_xgb_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "By CV, parameters of RForest:\n",
      "max_features set to 37\n",
      "n_estimators set to 600\n",
      "**************************************************\n",
      "Random Forest Training Performace: R^2 = 0.9830\n",
      "**************************************************\n",
      "Random Forest Training Performace: RMSE = 0.0521\n",
      "**************************************************\n",
      "Random Forest Test Performace: R^2 = 0.9830\n",
      "**************************************************\n",
      "Random Forest Test Performace: RMSE = 0.1289\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import math\n",
    "\n",
    "#rf_tree_cv = RandomForestRegressor(\n",
    "#                                bootstrap = True,\n",
    "#                                oob_score = True,\n",
    "#                                random_state = 0)\n",
    "\n",
    "#rf_param_grid = [{\n",
    "#        'n_estimators' : [300, 400, 500, 600], #Test[100,250,500,750,1000]\n",
    "#        'max_features' : np.arange(35,38), # First try: np.arange(17,40)\n",
    "#}]\n",
    "\n",
    "#rf_grid_search = GridSearchCV(rf_tree_cv, param_grid = rf_param_grid, cv = 5);\n",
    "\n",
    "#rf_grid_search.fit(X_train_v, y_train_v);\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "#max_features0_RForest = rf_grid_search.best_params_['max_features'];\n",
    "#n_estimators0_RForest = rf_grid_search.best_params_['n_estimators'];\n",
    "\n",
    "max_features0_RForest = 37;\n",
    "n_estimators0_RForest = 600;\n",
    "\n",
    "print('*'*50)\n",
    "print('By CV, parameters of RForest:')\n",
    "print('max_features set to {}'.format(max_features0_RForest))\n",
    "print('n_estimators set to {}'.format(n_estimators0_RForest))\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "#rf_tree = RandomForestRegressor(n_estimators = n_estimators0_RForest,\n",
    "#                                max_features = max_features0_RForest,\n",
    "#                                bootstrap = True,\n",
    "#                                oob_score = True,\n",
    "#                                random_state = 0)\n",
    "\n",
    "#rf_tree.fit(X_train_v, y_train_v)\n",
    "\n",
    "model_RForest = RandomForestRegressor(n_estimators = n_estimators0_RForest,\n",
    "                                      max_features = max_features0_RForest,\n",
    "                                      bootstrap = True,\n",
    "                                      oob_score = True,\n",
    "                                      random_state = 0)\n",
    "\n",
    "model_RForest = model_RForest.fit(X_train_v, y_train_v)\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "#R2_RForest_train = rf_tree.score(X_train_v, y_train_v);\n",
    "#RMSE_RForest_train = np.sqrt(mean_squared_error(y_train_v, rf_tree.predict(X_train_v)));\n",
    "\n",
    "R2_RForest_train = model_RForest.score(X_train_v, y_train_v);\n",
    "RMSE_RForest_train = np.sqrt(mean_squared_error(y_train_v, \n",
    "                                                model_RForest.predict(X_train_v)));\n",
    "\n",
    "print('*'*50)\n",
    "print('Random Forest Training Performace: R^2 = {:.4f}'.format(R2_RForest_train))\n",
    "print('*'*50)\n",
    "print('Random Forest Training Performace: RMSE = {:.4f}'.format(RMSE_RForest_train))\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "#R2_RForest_test = rf_tree.score(X_train_v, y_train_v);\n",
    "#RMSE_RForest_test = np.sqrt(mean_squared_error(y_test_v, rf_tree.predict(X_test_v)));\n",
    "\n",
    "R2_RForest_test = model_RForest.score(X_train_v, y_train_v);\n",
    "RMSE_RForest_test = np.sqrt(mean_squared_error(y_test_v, \n",
    "                                               model_RForest.predict(X_test_v)));\n",
    "\n",
    "print('*'*50)\n",
    "print('Random Forest Test Performace: R^2 = {:.4f}'.format(R2_RForest_test))\n",
    "print('*'*50)\n",
    "print('Random Forest Test Performace: RMSE = {:.4f}'.format(RMSE_RForest_test))\n",
    "print('*'*50)\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "#output_RForest_test = -1 + np.exp(rf_tree.predict(X_test));\n",
    "output_RForest_test = -1 + np.exp(model_RForest.predict(X_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Final Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Simple Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Simple Stacking:\n",
      "**************************************************\n",
      "Weight_Lasso: 0.3253\n",
      "**************************************************\n",
      "Weight_XGB: 0.4239\n",
      "**************************************************\n",
      "Weight_RForest: 0.2508\n",
      "**************************************************\n",
      "\n",
      "\n",
      "Final Predicted SalePrices:\n",
      "1459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 123724.69765036,  157999.72229538,  184744.92356571, ...,\n",
       "        159573.54661891,  116957.57815469,  220335.00004869])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use RMSE or use RMSE_to_R2\n",
    "\n",
    "use_RMSE = 0;\n",
    "# Use RMSE: use_RMSE = 1\n",
    "# Use RMSE_to_R2: use_RMSE = any number other than 1\n",
    "\n",
    "if use_RMSE == 1:\n",
    "    rho_Lasso = RMSE_Lasso_test;\n",
    "    rho_xgb = RMSE_Lasso_test;\n",
    "    rho_RForest = RMSE_Lasso_test;\n",
    "else:\n",
    "    rho_Lasso = RMSE_Lasso_test / R2_Lasso_test;\n",
    "    rho_xgb = RMSE_xgb_test / R2_xgb_test;\n",
    "    rho_RForest = RMSE_RForest_test / R2_RForest_test;\n",
    "    \n",
    "s = rho_Lasso + rho_xgb + rho_RForest;\n",
    "\n",
    "w_Lasso = 1 - rho_Lasso / s;\n",
    "w_xgb = 1 - rho_xgb / s;\n",
    "w_RForest = 1 - rho_RForest / s;\n",
    "\n",
    "output_final_SimpleStacking = 1/2 * (w_Lasso * output_Lasso_test + \n",
    "                                     w_xgb * output_xgb_test + \n",
    "                                     w_RForest * output_RForest_test)\n",
    "\n",
    "\n",
    "print('*'*50)\n",
    "print('Simple Stacking:')\n",
    "print('*'*50)\n",
    "print('Weight_Lasso: {:.4f}'.format(w_Lasso/2))\n",
    "print('*'*50)\n",
    "print('Weight_XGB: {:.4f}'.format(w_xgb/2))\n",
    "print('*'*50)\n",
    "print('Weight_RForest: {:.4f}'.format(w_RForest/2))\n",
    "print('*'*50)\n",
    "\n",
    "print('\\n')\n",
    "print('Final Predicted SalePrices:')\n",
    "print(len(output_final_SimpleStacking))\n",
    "output_final_SimpleStacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Fancy Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print('*'*70)\n",
    "#print(model_Lasso)\n",
    "#print('*'*70)\n",
    "#print(model_xgb)\n",
    "#print('*'*70)\n",
    "#print(model_RForest)\n",
    "#print('*'*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class stacked_meta_model():\n",
    "    \n",
    "    def __init__(self, base_models, meta_model, k_folds=num_of_folds):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.k_folds = k_folds\n",
    "   \n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # The class method .fit() defined below is\n",
    "    # the core of the stacked_meta_model!!\n",
    "    # -------------------------------------------------------------\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # -----------------------------------------\n",
    "        # Stage 1: Re-train/fit base models\n",
    "        # -----------------------------------------\n",
    "    \n",
    "        # Initialize list of re-fitted base_models:\n",
    "        self.base_models_Beta = [list() for i in self.base_models]\n",
    "        \n",
    "        # Prepare splitting for k-fold CV\n",
    "        k_f = KFold(n_splits=self.k_folds, shuffle=True, random_state=RandSeed0)\n",
    "        \n",
    "        # Re-Train base-models and then generate out-of-fold predictions.\n",
    "        oof_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        ## \"oof\" stands for out-of-fold\n",
    "        \n",
    "        for Model_Counter, Model in enumerate(self.base_models):\n",
    "            \n",
    "            for Folder_Counter, (tr_index, te_index) in enumerate(k_f.split(X, y)):\n",
    "                # X splits to X[tr_index] and X[te_indx]; likewise for y\n",
    "\n",
    "                # \"Deep-copy\" each original base model;\n",
    "                # Otherwise, doing fit will mutate the original model\n",
    "                instance = clone(Model)\n",
    "                                \n",
    "                ## For debugging purpose:\n",
    "                ##print(X.loc[tr_index])\n",
    "                ##print(y[tr_index])\n",
    "                ##print(instance) \n",
    "                ##print(type(tr_index))\n",
    "                \n",
    "                instance.fit(X.loc[tr_index], y[tr_index])\n",
    "                self.base_models_Beta[Model_Counter].append(instance)\n",
    "                \n",
    "                oof_predictions[te_index, Model_Counter] = \\\n",
    "                    instance.predict(X.loc[te_index])\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # Stage 2: Re-train/fit the meta model\n",
    "        # -----------------------------------------\n",
    "                \n",
    "        # \"Deep-copy\" the original meta model;\n",
    "        # Otherwise, doing fit will mutate the original meta model\n",
    "        self.meta_model_Beta = clone(self.meta_model)\n",
    "        \n",
    "        self.meta_model_Beta.fit(oof_predictions, y)\n",
    "        \n",
    "        #print(self.base_models_Beta)\n",
    "        #print('*'*50)\n",
    "        #print(self.meta_model_Beta)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "   \n",
    "\n",
    "    # Use the Meta Model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([Model.predict(X) for Model in Base_Models]).mean(axis=1)\n",
    "            for Base_Models in self.base_models_Beta])\n",
    "        return self.meta_model_Beta.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************************\n",
      "Stacked Meta Model Training Performace: R^2 = 0.9311\n",
      "************************************************************\n",
      "Stacked Meta Model Training Performace: RMSE = 0.1048\n",
      "************************************************************\n",
      "Stacked Meta Test Performace: R^2 = 0.9560\n",
      "************************************************************\n",
      "Stacked Meta Test Performace: RMSE = 0.0823\n",
      "************************************************************\n",
      "\n",
      "\n",
      "Final Predicted SalePrices\n",
      "1459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 116715.7421875,  164863.21875  ,  160508.71875  , ...,\n",
       "        159756.4375   ,  120283.2265625,  212363.59375  ], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_meta = stacked_meta_model(base_models=[model_Lasso, model_xgb, model_RForest], \n",
    "                                meta_model=model_xgb);\n",
    "\n",
    "model_meta.fit(X_train, y_train);\n",
    "\n",
    "\n",
    "predictions_meta_train = pd.DataFrame({\"Predicted\":model_meta.predict(X_train), \n",
    "                                       \"Actual\":y_train});\n",
    "predictions_meta_train[\"Residual\"] = predictions_meta_train.Actual - predictions_meta_train.Predicted;\n",
    "\n",
    "\n",
    "R2_meta_train = R2(predictions_meta_train.Predicted, predictions_meta_train.Actual);\n",
    "RMSE_meta_train = rmse(predictions_meta_train.Actual, predictions_meta_train.Predicted);\n",
    "\n",
    "print('*'*60)\n",
    "print('Stacked Meta Model Training Performace: R^2 = {:.4f}'.format(R2_meta_train))\n",
    "print('*'*60)\n",
    "print('Stacked Meta Model Training Performace: RMSE = {:.4f}'.format(RMSE_meta_train))\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Test Performance\n",
    "predictions_meta_test = pd.DataFrame({\"Predicted\":model_meta.predict(X_test_v), \n",
    "                                      \"Actual\":y_test_v});\n",
    "predictions_meta_test[\"Residual\"] = predictions_meta_test.Actual - predictions_meta_test.Predicted;\n",
    "\n",
    "\n",
    "R2_meta_test = R2(predictions_meta_test.Predicted, predictions_meta_test.Actual);\n",
    "RMSE_meta_test = rmse(predictions_meta_test.Actual, predictions_meta_test.Predicted);\n",
    "\n",
    "print('*'*60)\n",
    "print('Stacked Meta Test Performace: R^2 = {:.4f}'.format(R2_meta_test));\n",
    "print('*'*60)\n",
    "print('Stacked Meta Test Performace: RMSE = {:.4f}'.format(RMSE_meta_test));\n",
    "print('*'*60)\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "output_final_MetaStacking = model_meta.predict(X_test);\n",
    "\n",
    "# de-Logarithm\n",
    "output_final_MetaStacking = -1 + np.exp(output_final_MetaStacking); \n",
    "\n",
    "print('\\n')\n",
    "print('Final Predicted SalePrices')\n",
    "print(len(output_final_MetaStacking))\n",
    "output_final_MetaStacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate CSV File for Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Predictions by simple stacking:\n",
      "**************************************************\n",
      "     Id      SalePrice\n",
      "0  1461  123724.697650\n",
      "1  1462  157999.722295\n",
      "2  1463  184744.923566\n",
      "3  1464  191859.466300\n",
      "4  1465  194154.741696\n",
      "**************************************************\n",
      "Predictions by meta stacking:\n",
      "**************************************************\n",
      "     Id      SalePrice\n",
      "0  1461  116715.742188\n",
      "1  1462  164863.218750\n",
      "2  1463  160508.718750\n",
      "3  1464  176721.234375\n",
      "4  1465  209795.625000\n"
     ]
    }
   ],
   "source": [
    "# Simple Stacking Predictions\n",
    "Predictions_simple = pd.read_csv('Datasets/test.csv');\n",
    "\n",
    "Predictions_simple['SalePrice'] = output_final_SimpleStacking;\n",
    "Predictions_simple = Predictions_simple[['Id','SalePrice']];\n",
    "\n",
    "print('*'*50)\n",
    "print('Predictions by simple stacking:')\n",
    "print('*'*50)\n",
    "print(Predictions_simple.head())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Meta Stacking Predictions\n",
    "Predictions_meta = pd.read_csv('Datasets/test.csv');\n",
    "\n",
    "Predictions_meta['SalePrice'] = output_final_MetaStacking;\n",
    "Predictions_meta = Predictions_meta[['Id','SalePrice']];\n",
    "\n",
    "print('*'*50)\n",
    "print('Predictions by meta stacking:')\n",
    "print('*'*50)\n",
    "print(Predictions_meta.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "Write as: Datasets/Predictions_Simple_Stacking_0312-104843.csv\n",
      "Read in: Datasets/Predictions_Simple_Stacking_0312-104843.csv\n",
      "     Id      SalePrice\n",
      "0  1461  123724.697650\n",
      "1  1462  157999.722295\n",
      "2  1463  184744.923566\n",
      "3  1464  191859.466300\n",
      "4  1465  194154.741696\n",
      "\n",
      "\n",
      "**********************************************************************\n",
      "Write as: Datasets/Predictions_Meta_Stacking_0312-104843.csv\n",
      "Read in: Datasets/Predictions_Meta_Stacking_0312-104843.csv\n",
      "     Id      SalePrice\n",
      "0  1461  116715.742188\n",
      "1  1462  164863.218750\n",
      "2  1463  160508.718750\n",
      "3  1464  176721.234375\n",
      "4  1465  209795.625000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "timestr = time.strftime(\"%m%d-%H%M%S\")\n",
    "\n",
    "# Save predictions by simple stacking ---------------------------------------------\n",
    "fileName_str = 'Datasets/Predictions_Simple_Stacking_'+timestr+'.csv';\n",
    "\n",
    "print('*'*70)\n",
    "print('Write as:', fileName_str)\n",
    "Predictions_simple.to_csv(fileName_str,index = False);\n",
    "\n",
    "# Double Check whether the CSV file is properly saved and ready for submission\n",
    "print('Read in:', fileName_str)\n",
    "print(pd.read_csv(fileName_str).head())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Save predictions by meta stacking ---------------------------------------------\n",
    "fileName_str = 'Datasets/Predictions_Meta_Stacking_'+timestr+'.csv';\n",
    "\n",
    "print('*'*70)\n",
    "print('Write as:', fileName_str)\n",
    "Predictions_meta.to_csv(fileName_str,index = False);\n",
    "\n",
    "# Double Check whether the CSV file is properly saved and ready for submission\n",
    "print('Read in:', fileName_str)\n",
    "print(pd.read_csv(fileName_str).head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
